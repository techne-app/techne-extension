import { 
  CreateExtensionServiceWorkerMLCEngine, 
  type ChatCompletionMessageParam,
  InitProgressReport,
  prebuiltAppConfig
} from "@mlc-ai/web-llm";

export interface ChatOptions {
  messages: ChatCompletionMessageParam[];
  config: {
    model: string;
    temperature: number;
    topP: number;
    maxTokens: number;
    stream: boolean;
  };
  onUpdate?: (message: string, chunk: string) => void;
  onFinish?: (message: string) => void;
  onError?: (error: string) => void;
}

interface LLMConfig {
  model: string;
  temperature: number;
  top_p: number;
  max_tokens: number;
  stream: boolean;
}

export class WebLLMClient {
  private engine: any = null;
  private initialized = false;
  private llmConfig?: LLMConfig;

  constructor() {
    // Initialize with default config
  }

  private async initModel(onUpdate?: (message: string, chunk: string) => void): Promise<void> {
    if (!this.llmConfig) {
      throw Error("llmConfig is undefined");
    }

    // Create engine config with Cache API (Chrome recommended for AI models)
    const engineConfig = {
      appConfig: {
        ...prebuiltAppConfig,
        useIndexedDBCache: false, // Use Cache API as recommended by Chrome
      },
      initProgressCallback: (report: InitProgressReport) => {
        onUpdate?.(report.text, report.text);
      }
    };

    this.engine = await CreateExtensionServiceWorkerMLCEngine(
      this.llmConfig.model,
      engineConfig
    );

    this.initialized = true;
  }

  private isDifferentConfig(config: LLMConfig): boolean {
    if (!this.llmConfig) {
      return true;
    }

    // Compare required fields
    if (this.llmConfig.model !== config.model) {
      return true;
    }

    // Compare optional fields
    if (this.llmConfig.temperature !== config.temperature ||
        this.llmConfig.top_p !== config.top_p ||
        this.llmConfig.max_tokens !== config.max_tokens) {
      return true;
    }

    return false;
  }

  async chat(options: ChatOptions): Promise<void> {
    // Convert config to LLMConfig format
    const config: LLMConfig = {
      model: options.config.model,
      temperature: options.config.temperature,
      top_p: options.config.topP,
      max_tokens: options.config.maxTokens,
      stream: options.config.stream
    };

    if (!this.initialized || this.isDifferentConfig(config)) {
      this.llmConfig = { ...(this.llmConfig || {}), ...config };
      
      try {
        await this.initModel(options.onUpdate);
      } catch (err: any) {
        let errorMessage = err.message || err.toString() || "";
        if (errorMessage === "[object Object]") {
          errorMessage = JSON.stringify(err);
        }
        console.error("Error while initializing the model", errorMessage);
        options?.onError?.(errorMessage);
        return;
      }
    }

    let reply: string | null = "";
    
    try {
      const completion = await this.engine.chat.completions.create({
        stream: options.config.stream,
        messages: options.messages,
        temperature: options.config.temperature,
        top_p: options.config.topP,
        max_tokens: options.config.maxTokens,
      });

      if (options.config.stream) {
        for await (const chunk of completion) {
          const delta = chunk.choices[0]?.delta.content;
          if (delta) {
            reply += delta;
            options.onUpdate?.(reply, delta);
          }
        }
      } else {
        reply = completion.choices[0].message.content;
        options.onUpdate?.(reply || "", reply || "");
      }

      if (reply) {
        options.onFinish?.(reply);
      } else {
        options.onError?.("Empty response generated by LLM");
      }
    } catch (err: any) {
      let errorMessage = err.message || err.toString() || "";
      if (errorMessage === "[object Object]") {
        errorMessage = JSON.stringify(err);
      }
      
      // Handle port disconnection errors with automatic reconnection
      if (errorMessage.includes('disconnected port') || 
          errorMessage.includes('port is null') ||
          errorMessage.includes('Attempting to use a disconnected port object')) {
        console.log('Port disconnected, attempting to reinitialize and retry');
        try {
          await this.reset();
          await this.initModel(options.onUpdate);
          // Retry the chat once after reconnection
          return this.chat(options);
        } catch (retryErr: any) {
          console.error("Failed to reconnect and retry:", retryErr);
          options.onError?.("Connection lost. Please try again.");
          return;
        }
      }
      
      console.error("Error in chatCompletion", errorMessage);
      options.onError?.(errorMessage);
    }
  }

  async reset(): Promise<void> {
    this.initialized = false;
    this.llmConfig = undefined;
    this.engine = null;
  }
}

export const webLLMClient = new WebLLMClient();