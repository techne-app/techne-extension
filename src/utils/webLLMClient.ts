import { 
  CreateExtensionServiceWorkerMLCEngine, 
  type ChatCompletionMessageParam,
  InitProgressReport,
  prebuiltAppConfig
} from "@mlc-ai/web-llm";

export interface ChatOptions {
  messages: ChatCompletionMessageParam[];
  config: {
    model: string;
    temperature: number;
    topP: number;
    maxTokens: number;
    stream: boolean;
  };
  onUpdate?: (message: string, chunk: string) => void;
  onFinish?: (message: string) => void;
  onError?: (error: string) => void;
  onModelLoadingStart?: () => void;
  onModelLoadingProgress?: (progress: number, text: string) => void;
  onModelLoadingComplete?: () => Promise<boolean>; // Return false to abort chat
}

interface LLMConfig {
  model: string;
  temperature: number;
  top_p: number;
  max_tokens: number;
  stream: boolean;
}

export class WebLLMClient {
  private engine: any = null;
  private initialized = false;
  private llmConfig?: LLMConfig;

  constructor() {
    // Initialize with default config
  }

  private async initModel(options?: {
    onUpdate?: (message: string, chunk: string) => void;
    onModelLoadingStart?: () => void;
    onModelLoadingProgress?: (progress: number, text: string) => void;
    onModelLoadingComplete?: () => Promise<boolean>;
  }): Promise<boolean> {
    if (!this.llmConfig) {
      throw Error("llmConfig is undefined");
    }

    options?.onModelLoadingStart?.();

    // Create engine config with Cache API (Chrome recommended for AI models)
    const engineConfig = {
      appConfig: {
        ...prebuiltAppConfig,
        useIndexedDBCache: false, // Use Cache API as recommended by Chrome
      },
      initProgressCallback: (report: InitProgressReport) => {
        // Only send progress to model loading callback, not to chat update
        options?.onModelLoadingProgress?.(report.progress || 0, report.text);
      }
    };

    this.engine = await CreateExtensionServiceWorkerMLCEngine(
      this.llmConfig.model,
      engineConfig
    );

    this.initialized = true;
    const shouldContinue = await options?.onModelLoadingComplete?.();
    return shouldContinue !== false;
  }

  private isDifferentConfig(config: LLMConfig): boolean {
    if (!this.llmConfig) {
      return true;
    }

    // Only compare model - other parameters don't require reloading
    if (this.llmConfig.model !== config.model) {
      return true;
    }

    // temperature, top_p, max_tokens, and stream are inference parameters
    // that don't require model reloading
    return false;
  }

  async chat(options: ChatOptions): Promise<void> {
    // Convert config to LLMConfig format
    const config: LLMConfig = {
      model: options.config.model,
      temperature: options.config.temperature,
      top_p: options.config.topP,
      max_tokens: options.config.maxTokens,
      stream: options.config.stream
    };

    if (!this.initialized || this.isDifferentConfig(config)) {
      console.log('ðŸ”„ Model needs initialization. Initialized:', this.initialized, 'Config different:', this.isDifferentConfig(config));
      console.log('ðŸ”§ Current config:', this.llmConfig);
      console.log('ðŸ”§ New config:', config);
      
      this.llmConfig = { ...(this.llmConfig || {}), ...config };
      
      try {
        const shouldContinue = await this.initModel({
          onUpdate: options.onUpdate,
          onModelLoadingStart: options.onModelLoadingStart,
          onModelLoadingProgress: options.onModelLoadingProgress,
          onModelLoadingComplete: options.onModelLoadingComplete
        });
        
        if (!shouldContinue) {
          // Model loading completed but chat should be aborted
          return;
        }
      } catch (err: any) {
        let errorMessage = err.message || err.toString() || "";
        if (errorMessage === "[object Object]") {
          errorMessage = JSON.stringify(err);
        }
        console.error("Error while initializing the model", errorMessage);
        options?.onError?.(errorMessage);
        return;
      }
    } else {
      console.log('âœ… Model already initialized and config unchanged, skipping initialization');
    }

    let reply: string | null = "";
    
    try {
      const completion = await this.engine.chat.completions.create({
        stream: options.config.stream,
        messages: options.messages,
        temperature: options.config.temperature,
        top_p: options.config.topP,
        max_tokens: options.config.maxTokens,
      });

      if (options.config.stream) {
        for await (const chunk of completion) {
          const delta = chunk.choices[0]?.delta.content;
          if (delta) {
            reply += delta;
            options.onUpdate?.(reply, delta);
          }
        }
      } else {
        reply = completion.choices[0].message.content;
        options.onUpdate?.(reply || "", reply || "");
      }

      if (reply) {
        options.onFinish?.(reply);
      } else {
        options.onError?.("Empty response generated by LLM");
      }
    } catch (err: any) {
      let errorMessage = err.message || err.toString() || "";
      if (errorMessage === "[object Object]") {
        errorMessage = JSON.stringify(err);
      }
      
      // Handle port disconnection errors with automatic reconnection
      if (errorMessage.includes('disconnected port') || 
          errorMessage.includes('port is null') ||
          errorMessage.includes('Attempting to use a disconnected port object')) {
        console.log('Port disconnected, attempting to reinitialize and retry');
        try {
          await this.reset();
          const shouldContinue = await this.initModel({
            onUpdate: options.onUpdate,
            onModelLoadingStart: options.onModelLoadingStart,
            onModelLoadingProgress: options.onModelLoadingProgress,
            onModelLoadingComplete: options.onModelLoadingComplete
          });
          
          if (!shouldContinue) {
            return;
          }
          
          // Retry the chat once after reconnection
          return this.chat(options);
        } catch (retryErr: any) {
          console.error("Failed to reconnect and retry:", retryErr);
          options.onError?.("Connection lost. Please try again.");
          return;
        }
      }
      
      console.error("Error in chatCompletion", errorMessage);
      options.onError?.(errorMessage);
    }
  }

  async reset(): Promise<void> {
    this.initialized = false;
    this.llmConfig = undefined;
    this.engine = null;
  }

  isModelLoaded(): boolean {
    return this.initialized && this.engine !== null;
  }

  getModelName(): string | undefined {
    return this.llmConfig?.model;
  }
}

export const webLLMClient = new WebLLMClient();